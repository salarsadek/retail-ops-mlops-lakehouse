from __future__ import annotations

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import joblib
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

from retail_ops_mlops.utils.config import ensure_dirs, load_config

logger = logging.getLogger(__name__)

DATASET_ID = "m5"


@dataclass
class TrainReport:
    pipeline: str
    dataset_id: str
    status: str
    started_at_utc: str
    finished_at_utc: str
    config_path: str
    gold_dir: str
    report_path: str
    model_path: str | None
    metrics_path_csv: str | None
    metrics_path_tex: str | None
    predictions_sample_csv: str | None
    n_rows: int | None
    n_train: int | None
    n_test: int | None
    horizon: int
    metrics: dict[str, float]
    notes: dict[str, Any]


def _require(path: Path, *, strict: bool) -> None:
    if strict and not path.exists():
        raise FileNotFoundError(f"Missing required input: {path}")


def run(
    config_path: str | Path = "configs/default.yaml",
    *,
    horizon: int = 28,
    force: bool = False,
    strict: bool = True,
) -> Path:
    """
    Train a simple baseline model on Gold sample sales table.

    Inputs (Gold):
      - dim_calendar.parquet
      - fact_sales_long_sample.parquet

    Outputs:
      - outputs/models/m5_ridge_baseline.joblib
      - outputs/tables/train_m5_metrics.{csv,tex}
      - outputs/tables/train_m5_predictions_sample.csv
      - outputs/reports/train_m5.json
    """
    cfg = load_config(config_path)
    ensure_dirs(cfg)

    started = datetime.now(timezone.utc)

    gold_dir = cfg["paths"]["data_processed"] / DATASET_ID / "gold"
    cal_path = gold_dir / "dim_calendar.parquet"
    sales_path = gold_dir / "fact_sales_long_sample.parquet"

    _require(cal_path, strict=strict)
    _require(sales_path, strict=strict)

    report_path = cfg["paths"]["outputs_reports"] / "train_m5.json"
    model_path = cfg["paths"]["outputs_models"] / "m5_ridge_baseline.joblib"
    metrics_csv = cfg["paths"]["outputs_tables"] / "train_m5_metrics.csv"
    metrics_tex = cfg["paths"]["outputs_tables"] / "train_m5_metrics.tex"
    preds_csv = cfg["paths"]["outputs_tables"] / "train_m5_predictions_sample.csv"

    notes: dict[str, Any] = {
        "model": "Ridge + OneHotEncoder (simple baseline)",
        "uses_gold_sample_table": True,
        "horizon_definition": "last N days per series used as test",
        "force_overwrite": force,
    }

    if model_path.exists() and not force:
        finished = datetime.now(timezone.utc)
        rep = TrainReport(
            pipeline="train_m5",
            dataset_id=DATASET_ID,
            status="exists",
            started_at_utc=started.isoformat(),
            finished_at_utc=finished.isoformat(),
            config_path=str(cfg["config_path"]),
            gold_dir=str(gold_dir),
            report_path=str(report_path),
            model_path=str(model_path),
            metrics_path_csv=str(metrics_csv) if metrics_csv.exists() else None,
            metrics_path_tex=str(metrics_tex) if metrics_tex.exists() else None,
            predictions_sample_csv=str(preds_csv) if preds_csv.exists() else None,
            n_rows=None,
            n_train=None,
            n_test=None,
            horizon=horizon,
            metrics={},
            notes=notes,
        )
        report_path.write_text(json.dumps(asdict(rep), indent=2), encoding="utf-8")
        return report_path

    cal = pd.read_parquet(cal_path)
    sales = pd.read_parquet(sales_path)

    # Expect at least: id, d, sales (others optional)
    required_cols = {"id", "d", "sales"}
    missing = required_cols - set(sales.columns)
    if missing:
        raise ValueError(f"Gold sales sample missing columns: {sorted(missing)}")

    # Join calendar features if available
    cal_keep = [
        c
        for c in ["d", "wm_yr_wk", "wday", "month", "year", "snap_CA", "snap_TX", "snap_WI"]
        if c in cal.columns
    ]
    if "d" in cal_keep and len(cal_keep) > 1:
        sales = sales.merge(cal[cal_keep], on="d", how="left")

    # Sort by numeric day index
    sales = sales.copy()
    sales["d_num"] = sales["d"].astype(str).str.replace("d_", "", regex=False).astype(int)
    sales = sales.sort_values(["id", "d_num"]).reset_index(drop=True)

    g = sales.groupby("id", sort=False)

    # Lag/rolling features
    for lag in (1, 7, 28):
        sales[f"lag_{lag}"] = g["sales"].shift(lag)

    sales["roll_mean_7"] = g["sales"].transform(lambda s: s.shift(1).rolling(7).mean())
    sales["roll_mean_28"] = g["sales"].transform(lambda s: s.shift(1).rolling(28).mean())

    # Train/test split: last horizon rows per series as test
    sales["idx_in_series"] = g.cumcount()
    sales["len_series"] = g["sales"].transform("size")
    sales["is_test"] = sales["idx_in_series"] >= (sales["len_series"] - int(horizon))

    feat_cols_num = [c for c in sales.columns if c.startswith("lag_")] + [
        "roll_mean_7",
        "roll_mean_28",
        "d_num",
    ]
    feat_cols_num += [
        c
        for c in ["wm_yr_wk", "wday", "month", "year", "snap_CA", "snap_TX", "snap_WI"]
        if c in sales.columns
    ]
    feat_cols_num = [c for c in feat_cols_num if c in sales.columns]

    # Optional categorical identifiers (if present)
    candidate_cat = ["id", "item_id", "dept_id", "cat_id", "store_id", "state_id"]
    feat_cols_cat = [c for c in candidate_cat if c in sales.columns]

    # Drop rows with NA in numeric features (due to lags/rolls)
    model_df = sales.dropna(subset=feat_cols_num).copy()

    train_df = model_df[~model_df["is_test"]].copy()
    test_df = model_df[model_df["is_test"]].copy()

    X_train = train_df[feat_cols_num + feat_cols_cat]
    y_train = train_df["sales"].astype(float)

    X_test = test_df[feat_cols_num + feat_cols_cat]
    y_test = test_df["sales"].astype(float)

    pre = ColumnTransformer(
        transformers=[
            ("num", "passthrough", feat_cols_num),
            ("cat", OneHotEncoder(handle_unknown="ignore"), feat_cols_cat),
        ],
        remainder="drop",
    )

    pipe = Pipeline(
        steps=[
            ("pre", pre),
            ("model", Ridge(alpha=1.0)),
        ]
    )

    logger.info("Training baseline Ridge on %d rows (test=%d)", len(train_df), len(test_df))
    pipe.fit(X_train, y_train)

    pred = pipe.predict(X_test)

    mae = float(mean_absolute_error(y_test, pred))
    rmse = float(mean_squared_error(y_test, pred, squared=False))

    # Save artifacts
    model_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(pipe, model_path)

    metrics_df = pd.DataFrame(
        [
            {"metric": "MAE", "value": mae},
            {"metric": "RMSE", "value": rmse},
            {"metric": "horizon", "value": float(horizon)},
            {"metric": "n_train", "value": float(len(train_df))},
            {"metric": "n_test", "value": float(len(test_df))},
        ]
    )
    metrics_df.to_csv(metrics_csv, index=False)
    metrics_df.to_latex(metrics_tex, index=False, float_format="%.4f")

    sample_preds = test_df[["id", "d", "d_num"]].copy()
    sample_preds["y_true"] = y_test.values
    sample_preds["y_pred"] = pred
    sample_preds = sample_preds.sort_values(["id", "d_num"]).head(200)
    sample_preds.to_csv(preds_csv, index=False)

    finished = datetime.now(timezone.utc)

    rep = TrainReport(
        pipeline="train_m5",
        dataset_id=DATASET_ID,
        status="ok",
        started_at_utc=started.isoformat(),
        finished_at_utc=finished.isoformat(),
        config_path=str(cfg["config_path"]),
        gold_dir=str(gold_dir),
        report_path=str(report_path),
        model_path=str(model_path),
        metrics_path_csv=str(metrics_csv),
        metrics_path_tex=str(metrics_tex),
        predictions_sample_csv=str(preds_csv),
        n_rows=int(len(model_df)),
        n_train=int(len(train_df)),
        n_test=int(len(test_df)),
        horizon=int(horizon),
        metrics={"mae": mae, "rmse": rmse},
        notes=notes,
    )

    report_path.write_text(json.dumps(asdict(rep), indent=2), encoding="utf-8")
    return report_path


if __name__ == "__main__":
    run()
